{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrate mechanisms within AWS SageMaker and Azure ML to deploy and serve machine learning models including:\n",
    "\n",
    "- API deployment\n",
    "- Safe rollout using A/B testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model deployment involves taking a trained machine learning model and making it available for real-time predictions or inferences. When it comes to deploying models, there are two primary outcomes to consider: APIs (Application Programming Interfaces) and edge deployment. These two modes offer distinct advantages and cater to different use cases.\n",
    "\n",
    "API deployment involves hosting the machine learning model on a server and exposing it as a service through an API. This allows clients or applications to send requests to the API and receive predictions or inferences in response. API deployment is ideal for scenarios where there is a centralized infrastructure and clients have reliable network connectivity. Examples of API deployment include using frameworks like Flask or FastAPI to build a RESTful API for deploying a natural language processing (NLP) model or an image recognition model.\n",
    "\n",
    "On the other hand, edge deployment brings the model closer to the data source or the client device itself, reducing latency and enabling real-time predictions without relying on a network connection. In this mode, the model runs directly on edge devices such as smartphones, IoT devices, or embedded systems. Edge deployment is particularly valuable in scenarios where low latency, privacy, or intermittent network connectivity is crucial. Examples of edge deployment include deploying a computer vision model on a surveillance camera to detect anomalies in real-time or deploying a speech recognition model on a smartphone for offline voice commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are APIs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common method to deploy a model on the web is to wrap the saved model as a API service and allow users (clients) to send requests. Incoming requests are parsed into the appropriate input format by the service and presented to the model for inference. This inference is returned to the user as a response. Each request is handled by a specific resource (in our case a model) that is identified by a unique *endpoint*. \n",
    "\n",
    "Think of an endpoint as the unique URL that is shared with the client to interface with the model; all they can do is to send a request to the endpoint (i.e., they have no access to any detail on how the response is generated). Intuitively, it is like a storefront (a unique address) where they come to collect their predictions. They do not worry about *how* the predictions are made. An endpoint separates the user-facing \"front end\" from a predictve model infused \"back end\". \n",
    "\n",
    "But what exactly is an Application Programming Interface (API)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APIs prescribe the mechanism through which any two computers can exchange information over a network. Given that there could be many ways to execute this exchange, it would be prudent to formalize this exchange as a set of rules that we agree. These rules are encoded as REST principles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rest-api](assets/rest-api.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REpresentational State Transfer (REST) APIs are programming language agnostic and encode a set of rules that constitute a REST-ful API. These rules are:\n",
    "\n",
    "- Clients can only make POST, GET, PUT, or DELETE requests\n",
    "- These requests can contain an optional payload (usually a [JSON object](https://www.json.org/json-en.html))\n",
    "- All requests should return a response with a code indicating the status of the response (200's - Success, 400's - Improper request, 500's - Server side errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models as APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of ML deployment, clients send a POST request with a payload containing the input data needed by the model to make a prediction. For example, to get a classification result on their input, showrooms should attach the features of a diamond as a payload and upload it to the unique URL encoded by the endpoint. The server parses this input, presents it to the model, collects the prediction and sends a response (along with a status code) back to the client. In sum, customers *post* an input and the business serves a response.\n",
    "\n",
    "Common web frameworks used in production that implement the REST framework in Python are [Flask](https://palletsprojects.com/p/flask/) and [FastAPI](https://fastapi.tiangolo.com/). Flask is a popular REST implementation that is used by [SageMaker](https://aws.amazon.com/blogs/machine-learning/part-2-model-hosting-patterns-in-amazon-sagemaker-getting-started-with-deploying-real-time-models-on-sagemaker/) & [Azure ML](https://liupeirong.github.io/amlDockerImage/) to [create a web server for ML models](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-inference-server-http?view=azureml-api-2). The advantage of Flask is that owing to its longer existence, it enjoys a wider ecosystem compared with FastAPI. Beyond these general purpose implementations, specialized implementations also exist. For example, [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) implements the REST framework in C++ for TensorFlow models and hence can be more performant for deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "<b>Business Context (Review)</b> \n",
    "    \n",
    "For this session consider the case of a popular diamond jeweller - Brilliant Earth - with 30 showrooms across the US facing a price prediction problem. A common customer question that echoes in their retail outlets is the impact on price because of changes in some aspects of the ornament. For example, usually customers ask: \"If I decreased the carat of the diamonds used in this design, by how much would the price reduce?\". Such queries often require an expert intervention on the shopfloor and result in a subdued customer experience. The company also wants to implement a price predictor tool on their website so customers can engage with the brand better. At the moment, no such tool exists and the business team estimates that a price predictor will improve traffic to the website and also improve the time spent on the website.\n",
    "\n",
    "The dataset used in this session is scraped from the [Brilliant Earth website](https://www.brilliantearth.com/) and hosted on [Open ML](https://www.openml.org/search?type=data&status=active&id=43355).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a fully fleshed out endpoint for the diamond price prediction problem is [here](https://pgurazada1-diamond-price-predictor.hf.space/). In the rest of this session, we present the details behind building a REST API with the models estimated at their core."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AWS imports & authentication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.sklearn.estimator import SKLearnModel\n",
    "\n",
    "from sagemaker.session import production_variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `sagemaker` session is a cloud equivalent to a fully functional local development setup (i.e., access enabled to data and compute). We can point a session to a default bucket that will host all the artifacts accessed and created during the session (remember nothing stays local). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_session = sagemaker.Session(\n",
    "    default_bucket=\"sagemaker-deployment-examples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    aws_role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    print(\"Config file not found on local machine, use SageMaker Studio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From within SageMaker studio, execution role is inherited. Outside the Studio environment, the execution role should be explictly specified. This execution role should have [AmazonSageMakerFullAccess](https://docs.aws.amazon.com/sagemaker/latest/dg/security-iam-awsmanpol.html) permissions. Local compute [access](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-files.html) should also be [enabled](https://stackoverflow.com/a/47767351)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"AWS execution role associated with the account {aws_role}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Azure imports & authentication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "from azure.ai.ml import Input\n",
    "from azure.ai.ml import command\n",
    "\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    CodeConfiguration\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"5bcad9c4-40fb-4136-b614-cc90116dd8b3\"\n",
    "resource_group = \"tf\"\n",
    "workspace = \"cloud-teach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\")\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From VMs within the Azure ML workspace, the default Azure credentials are inherited. However, interactive browser credentials could be used to authenticate an Azure account to the Azure ML workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_credentials = DefaultAzureCredential(\n",
    "    exclude_interactive_browser_credential=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client = MLClient(\n",
    "    az_credentials, subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = pd.read_csv('s3://sagemaker-ap-south-1-321112151583/prices/diamond-prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for registered_data in ml_client.data.list():\n",
    "    print(registered_data.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamond_prices_data = ml_client.data.get(\n",
    "    name=\"diamond-prices-jan\",\n",
    "    version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df = pd.read_csv(diamond_prices_data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamonds_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate two models for the diamond prices data - a decision tree regressor (`dt.py`) and a gradient boosted regressor (`gb.py`).\n",
    "\n",
    "The input data is hosted in the default S3 bucket of the `sagemaker` session as an unprocessed csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_dt_estimator = SKLearn(\n",
    "    entry_point=\"aws/train/dt.py\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    volume_size=1,\n",
    "    role=aws_role,\n",
    "    sagemaker_session=deployment_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_dt_estimator.fit(\n",
    "    inputs={\n",
    "    'train': 's3://sagemaker-ap-south-1-321112151583/prices/'\n",
    "    },\n",
    "    wait=False,\n",
    "    job_name='2023-06-12-estimate-dt-003'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_dt_estimator.logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_gb_estimator = SKLearn(\n",
    "    entry_point=\"aws/train/gb.py\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    role=aws_role,\n",
    "    sagemaker_session=deployment_session,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    volume_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_gb_estimator.fit(\n",
    "    inputs={\n",
    "    'train': 's3://sagemaker-ap-south-1-321112151583/prices/'\n",
    "    },\n",
    "    wait=False,\n",
    "    job_name='2023-06-12-estimate-gb-003'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_gb_estimator.logs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two key aspects of the training scripts (`dt.py` and `gb.py`) that are new here:\n",
    "\n",
    "**1. The training workflow is encapsulated within a \"main guard\"** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows the training modules to be executed only when the training script is called from the command line. This is a good practise to ensure that the training process does not execute when the script is used as a part of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Model pipelines are estimated rather than the models themselves**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "preprocessor = make_column_transformer(\n",
    "        (StandardScaler(), numeric_features),\n",
    "        (OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    ")\n",
    "\n",
    "model_dt = DecisionTreeRegressor()\n",
    "\n",
    "model_pipeline = make_pipeline(preprocessor, model_dt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By estimating a preprocessing pipeline along with the model, we ensure that the data processing is \"packaged\" along with the model estimation. This is a good practise if the preprocessing involves standard, light-weight steps. Extensive preprocessing steps are best handled through a pipeline job. This way we avoid potentially costly data transfers between two steps - pre-processing and model estimation. Packaging preprocessing wth the model estimation also helps complex pipeline patterns during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output from the training script is persisted to the bucket allocated for the training job within the `output` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_dt_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_gb_estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this stage, we could have extracted the best model through hyperparameter tuning. However, for the purpose of model deployment, we are only concerned with obtaining the final model file that represents the best model for the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_train_job = command(\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_file\", path=\"azureml:diamond-prices-jan:1\")\n",
    "    },\n",
    "    code=\"azure/train/dt.py\",\n",
    "    command=\"python dt.py --data ${{inputs.data}}\",\n",
    "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
    "    display_name=\"2023-06-12-decision-tree-regression-example-003\",\n",
    "    experiment_name=\"2023-06-12-estimate-dt-003\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.create_or_update(dt_train_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_train_job = command(\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_file\", path=\"azureml:diamond-prices-jan:1\")\n",
    "    },\n",
    "    code=\"azure/train/gb.py\",\n",
    "    command=\"python gb.py --data ${{inputs.data}}\",\n",
    "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
    "    display_name=\"2023-06-12-gradient-boosting-regression-example-003\",\n",
    "    experiment_name=\"2023-06-12-estimate-gb-003\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.create_or_update(gb_train_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.get(\"modest_salt_hvgbf8s4sk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.get(\"elated_seal_7h7br1q8z1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three key aspects of the training scripts (`dt.py` and `gb.py`) that are new here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. The training workflow is encapsulated within a \"main guard\"** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows the training modules to be executed only when the training script is called from the command line. This is a good practise to ensure that the training process does not execute when the script is used as a part of a larger pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Model pipelines are estimated rather than the models themselves**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "preprocessor = make_column_transformer(\n",
    "        (StandardScaler(), numeric_features),\n",
    "        (OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    ")\n",
    "\n",
    "model_dt = DecisionTreeRegressor()\n",
    "\n",
    "model_pipeline = make_pipeline(preprocessor, model_dt)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By estimating a preprocessing pipeline along with the model, we ensure that the data processing is \"packaged\" along with the model estimation. This is a good practise if the preprocessing involves standard, light-weight steps. Extensive preprocessing steps are best handled through a pipeline job. This way we avoid potentially costly data transfers between two steps - pre-processing and model estimation. Packaging preprocessing wth the model estimation also helps complex pipeline patterns during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Given the deep integration of `mlflow` within Azure ML, we can log and register models during the estimation process itself**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "mlflow.sklearn.log_model(\n",
    "        sk_model=model_pipeline,\n",
    "        registered_model_name=\"gbr-diamond-price-predictor-june\",\n",
    "        artifact_path=\"diamond-price-predictor\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage here is that if a model with the registered name exists within the Azure ML workspace, it automatically gets updated with a new version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating an Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the gradient boosted model has a better R-squared, let us deploy the gradient boosted model as the first version of the diamond price predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register a `Model` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a container image**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SKLearnModel` class allows you to package and deploy your scikit-learn model on SageMaker easily. Beyond specifying the location of the model artifacts, to register a model, we need to specify the `entry_point` parameter when creating an instance of the `SKLearnModel` class. The `entry_point` refers to the Python script that contains the code for inference, which is responsible for loading the model and making predictions. By specifying the `entry_point` parameter and providing the inference script, we ensure that SageMaker can correctly load the model and invoke the necessary functions during the deployment process. This allows SageMaker to set up the underlying infrastructure, create the endpoint, and handle the incoming prediction requests using our scikit-learn model.\n",
    "\n",
    "Under the hood, the `SKLearnModel` class takes care of packaging your model and the provided inference script into a deployable container image. This container image contains the necessary runtime dependencies, environment, and the specified entry point for the inference script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_dt_estimator.model_data, sklearn_gb_estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb = SKLearnModel(\n",
    "    model_data=sklearn_gb_estimator.model_data,\n",
    "    entry_point=\"aws/infer/inference.py\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    role=aws_role,\n",
    "    sagemaker_session=deployment_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt = SKLearnModel(\n",
    "    model_data=sklearn_dt_estimator.model_data,\n",
    "    entry_point=\"aws/infer/inference.py\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    role=aws_role,\n",
    "    sagemaker_session=deployment_session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The inference script**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inference.py` script plays a crucial role in guiding the `sagemaker` model server for handling inputs and generating model predictions. SageMaker provides explicit guidelines on the specific functions within this script that will be invoked when a prediction request is received. \n",
    "\n",
    "The `inference.py` file contains detailed comments that provide a clear understanding of the purpose and functionality of each function in the script. To provide a concise overview of the functions in the inference script, refer to the figure below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![aws-inference](assets/aws-inference.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrastructure and Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the server logic is implemented in the inference script, we define the infrastructure we need to host and serve the model. SageMaker handles the resources needed to create the model server and generates an endpoint with the name specified. In this process, it uses the container image created in the previous two steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_gb = model_gb.deploy(\n",
    "    endpoint_name='diamond-price-gb',\n",
    "    instance_type=\"ml.m5.xlarge\", \n",
    "    initial_instance_count=1,\n",
    "    wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the endpoints created in the previous step, we collect test data as traffic and present it to the end points.  This helps iron out potential errors before the endpoint is rolled out to customers. Usually, data that the model has never seen before is used to test deployments (we look at monitoring endpoints in further detail in the next session).\n",
    "\n",
    "The input type for a prediction request to our model as defined in `inference.py` is `csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = diamonds_df.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = ['carat']\n",
    "categorical_features = ['shape', 'cut', 'color', 'clarity', 'report', 'type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = numeric_features + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_Xtest = sample_df[features]\n",
    "sample_ytest = sample_df['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_Xtest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that at this point the endpoints are in service but are not publicly accessible. However, these endpoint can be invoked within the domain using the `sagemaker` runtime. As the code below indicates, we create a temporary `csv` file from the sample data frame created in the previous step to be presented to the corresponding endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the response from the Gradient Boosted Regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=predictor_gb.endpoint_name,\n",
    "    Body=sample_Xtest.to_csv(header=True, index=False).encode(\"utf-8\"),\n",
    "    ContentType=\"text/csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that the endpoint is REST-ful, we can check the status code of its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response['ResponseMetadata']['HTTPStatusCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"Body\"].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare this response with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have a model that can receive external traffic. However, there are further steps to go before a full rollout happens. To avoid costs incurred on idle endpoints during the testing phase, it is a good practise to delete end points. Production end points should ideally be generated and maintained by a separate team (even if they are using the same code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_gb.delete_endpoint(delete_endpoint_config=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_endpoint_name = \"diamond-price-predictor-001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"Model to predict diamond prices\",\n",
    "    auth_mode=\"aml_token\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By creating a `ManagedOnlineEndpoint` we let Azure handle all the resource creation and management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_gb = ml_client.models.get(\n",
    "    name=\"gbr-diamond-price-predictor-june\", \n",
    "    version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_gb.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare a scoring script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring scrips guide the Azure ML model server on input handling and generating model predictions. Azure ML defines clear guidelines on the functions within this script that will be invoked when a prediction request is received (the file `score.py` presents detailed comments that delineate what each function in the script accomplishes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![azure-score](assets/azure-score.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infrastructure & Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base model that we will deploy is referred to as the \"blue\" model by convention. After creation, this endpoint is intended to serve 100% of the traffic with the variant tagged as the blue version (the gradient boosted model in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the server logic is implemented in the scoring script, we define the infrastructure we need to host and serve the model. Azure ML handles the resources needed to create the model server and attaches it to the endpoint with the name specified (note that the managed endpoint was created in the first step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=registered_model_gb,\n",
    "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code='./azure/infer',\n",
    "        scoring_script='score.py'\n",
    "    ),\n",
    "    instance_type=\"Standard_DS1_v2\",\n",
    "    instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(diamonds_df.drop(columns='price')\n",
    "            .sample(100)\n",
    "            .to_json('sample-data.json', orient='split', lines=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    ml_client.online_endpoints.invoke(\n",
    "        endpoint_name=online_endpoint_name,\n",
    "        deployment_name=\"blue\",\n",
    "        request_file=\"sample-data.json\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canary Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important scenario in model deployment is the need to upgrade an existing baseline model to a newer version. To ensure a careful transition from the existing model to the new version, a recommended approach is through a canary deployment. This method involves directing a controlled portion of the live traffic to the upgraded endpoint, followed by A/B testing to determine if the upgraded version performs better than the baseline on live data.\n",
    "\n",
    "The canary deployment process starts by diverting a small percentage of live traffic, typically between 1% and 5%, to the upgraded version. Gradually, the traffic is increased if there are no errors. This approach allows for incremental testing and monitoring of the new model's performance in a real-world environment.\n",
    "\n",
    "Let's take a closer look at how canary deployment works in action. We begin by creating two model variants, each representing one of the two models we estimated on the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create variants from the model binaries, we reference the container configuration used by the `SKLearnModel` objects. Containerization is a popular method to package the model and server, along with all the runtime requirements, into a standalone resource. This approach ensures that the server can be deployed easily on any virtual machine without the need for manual duplication of the configuration options required to run the server.\n",
    "\n",
    "There are popular containerization tools available that allow us to quickly package all the runtime requirements into a reusable container. Two commonly used tools are [Docker](https://www.docker.com/) and [Podman](https://podman.io/). These tools simplify the process of creating containers, making it easier to manage and deploy the model and server components as a single unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt.prepare_container_def()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gb.prepare_container_def()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above output indicates, both model objects reference an image `720646828776.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3` that is managed by AWS. By building this image into a container we get an environment where Python 3, scikit-learn 1.2.1 and its dependencies (e.g., numpy and scipy) are preinstalled. When this container is run, we get a python runtime that executes the script `inference.py` with all its requirements (i.e., packages and model data) copied over to this runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the information on the infrastructure the model needs to fire predictions, we can register the two model binaries against a common endpoint as variants using the corresponding container configurations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by registering the models and their container environments within the current session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_session.create_model(\n",
    "    name='decision-tree-regressor',\n",
    "    role=aws_role,\n",
    "    container_defs=model_dt.prepare_container_def()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_session.create_model(\n",
    "    name='gradient-boosted-regressor',\n",
    "    role=aws_role,\n",
    "    container_defs=model_gb.prepare_container_def()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create two variants by referencing these two registered models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant1 = production_variant(\n",
    "    model_name='decision-tree-regressor',\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    variant_name=\"Variant1\",\n",
    "    initial_weight=0.95,\n",
    "    volume_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant2 = production_variant(\n",
    "    model_name='gradient-boosted-regressor',\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    initial_instance_count=1,\n",
    "    variant_name=\"Variant2\",\n",
    "    initial_weight=0.05,\n",
    "    volume_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(variant1, variant2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we note above, initially the two variants are configured to receive 95% (decision tree regressor) and 5% (gradient boosted regressor) respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can deploy the variants against the same endpoint allowing `sagemaker` to route incoming traffic in the ratio 95% and 5% to the two variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canary_endpoint_name = \"diamond-price-pred-2023-06-12\"\n",
    "print(f\"EndpointName = {canary_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_session.endpoint_from_production_variants(\n",
    "    name=canary_endpoint_name, \n",
    "    production_variants=[variant1, variant2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify the specification of the canary endpoint from the UI to ensure that the traffic flow is correctly configured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for invocation_num in range(100):\n",
    "    \n",
    "    sample_df = diamonds_df.sample(1)\n",
    "    sample_Xtest = sample_df[features]\n",
    "    \n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=canary_endpoint_name,\n",
    "        Body=sample_Xtest.to_csv(header=True, index=False).encode(\"utf-8\"),\n",
    "        ContentType=\"text/csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the traffic allocation patterns by looking at the invocation traffic to the endpoint on CloudWatch (expect a slight lag for data to be logged)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safe rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the updated variant is tested, we can slowly increase the weights assigned to the upgrade gradually pushing all the traffic over to the new variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client = boto3.Session().client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.update_endpoint_weights_and_capacities(\n",
    "    EndpointName=canary_endpoint_name,\n",
    "    DesiredWeightsAndCapacities=[\n",
    "        {'VariantName': 'Variant1', 'DesiredWeight': 0.8},\n",
    "        {'VariantName': 'Variant2', 'DesiredWeight': 0.2}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for invocation_num in range(100):\n",
    "    \n",
    "    sample_df = diamonds_df.sample(1)\n",
    "    sample_Xtest = sample_df[features]\n",
    "    \n",
    "    response = runtime.invoke_endpoint(\n",
    "        EndpointName=canary_endpoint_name,\n",
    "        Body=sample_Xtest.to_csv(header=True, index=False).encode(\"utf-8\"),\n",
    "        ContentType=\"text/csv\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have a `blue` variant for the gradient boosted model, let us create a `green` by referencing the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_dt = ml_client.models.get(\n",
    "    name=\"dt-diamond-price-predictor-june\", \n",
    "    version=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_dt.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Green deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `green` deployment is exactly the same as the blue deployment, except for the change in the variant name and the model used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_deployment = ManagedOnlineDeployment(\n",
    "    name=\"green\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=registered_model_dt,\n",
    "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code='./azure/infer',\n",
    "        scoring_script='score.py'\n",
    "    ),\n",
    "    instance_type=\"Standard_DS1_v2\",\n",
    "    instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(green_deployment).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, even though the endpoint is aware of a \"green\" version and we can invoke it, it is not yet receiving public traffic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    deployment_name=\"green\",\n",
    "    request_file=\"sample-data.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safe rollout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the green variant is tested, we can define the traffic proportions to be allocated dynamically, gradually increasing the traffic seen by the green endpoint, eventually rolling over completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.traffic = {\"blue\": 99, \"green\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    ml_client.online_endpoints.invoke(\n",
    "        endpoint_name=online_endpoint_name,\n",
    "        request_file=\"sample-data.json\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid costs incurred on idle endpoints during the testing phase, it is a good practise to delete end points. Production end points should ideally be generated and maintained by a separate team (even if they are using the same code). Data Science teams should not have edit access to production endpoints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_session.delete_endpoint(canary_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_session.delete_endpoint_config(canary_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in ['decision-tree-regressor', 'gradient-boosted-regressor']:\n",
    "    deployment_session.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
