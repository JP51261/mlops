{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "# MLS-7"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "### Connect to the workspace\n",
        "\n",
        "First, we'll need to connect to your Azure Machine Learning workspace. The Azure Machine Learning workspace is the top-level resource for the service. It provides you with a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n",
        "\n",
        "We're using DefaultAzureCredential to get access to the workspace. This credential should be capable of handling most Azure SDK authentication scenarios."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "from azure.ai.ml import command, Input\n",
        "\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    CodeConfiguration\n",
        ")\n",
        "\n",
        "from azure.storage.blob import BlobServiceClient"
      ],
      "outputs": [],
      "execution_count": 111,
      "metadata": {
        "gather": {
          "logged": 1694792908023
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "Next, get a handle to the workspace by providing your Subscription ID, Resource Group name, and workspace name. To find these parameters:\n",
        "\n",
        "* Look in the upper-right corner of the Azure Machine Learning studio toolbar for your workspace name.\n",
        "* Select your workspace name to show your Resource Group and Subscription ID.\n",
        "* Copy the values for Resource Group and Subscription ID into the code."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"1c6d9ef7-1867-436f-986f-f37c475a295b\", #Provide your subscription ID as shown in the above screenshot\n",
        "    resource_group_name=\"testml\", #Provide your Resource Group as shown in the above screenshot\n",
        "    workspace_name=\"azureml\",\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 112,
      "metadata": {
        "gather": {
          "logged": 1694792909211
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "### Create a compute resource to run the job\n",
        "\n",
        "Azure Machine Learning needs a compute resource to run a job. This resource can be single or multi-node machines with Linux or Windows OS, or a specific compute fabric like Spark.\n",
        "\n",
        "We only need a basic cluster for this task; thus, we'll pick a Standard_D2_v3 model with 2 CPU cores and 7 GB RAM to create an Azure Machine Learning compute.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "cpu_compute_target = \"cpu-cluster\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure ML compute object with the intended parameters\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        # Azure ML Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"STANDARD_D2_V3\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=1,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster).result()\n",
        "\n",
        "print(\n",
        "    f\"AMLCompute with name {cpu_cluster.name} is created, the compute size is {cpu_cluster.size}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named cpu-cluster, we'll reuse it as is.\nAMLCompute with name cpu-cluster is created, the compute size is STANDARD_D2_V3\n"
        }
      ],
      "execution_count": 113,
      "metadata": {
        "gather": {
          "logged": 1694792910496
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "### Configure and submit your training job"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Prepare the training script"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To use the training script, first create a directory where you will store the file.\n",
        "import os\n",
        "\n",
        "src_dir = \"./src\"\n",
        "os.makedirs(src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 114,
      "metadata": {
        "gather": {
          "logged": 1694792912748
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "**Next, create the script file in the source directory**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {src_dir}/main.py\n",
        "\n",
        "import os\n",
        "import mlflow\n",
        "import argparse\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "def main():\n",
        "\n",
        "    mlflow.start_run() # Start an MLflow run\n",
        "\n",
        "    parser = argparse.ArgumentParser() # Create an argument parser\n",
        "    parser.add_argument(\"--data\", type=str)  # Add an argument for the data file path\n",
        "    parser.add_argument(\"--learning-rate\", required=False, default=0.1, type=float)\n",
        "    parser.add_argument(\"--max-depth\", required=False, default=4, type=float)\n",
        "    parser.add_argument(\"--n-estimators\", required=False, default=100, type=int)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    airbnb = pd.read_csv(args.data)\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X = airbnb.drop(columns=[\"price\"])\n",
        "    X = pd.get_dummies(X,drop_first=True)\n",
        "\n",
        "    y = airbnb[\"price\"]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    # Create a Gradient Boosting Regressor model with the specified hyperparameters\n",
        "    model_gbr = GradientBoostingRegressor(\n",
        "        n_estimators=args.n_estimators,\n",
        "        learning_rate=args.learning_rate,\n",
        "        max_depth=args.max_depth\n",
        "    )\n",
        "\n",
        "\n",
        "    model_gbr.fit(X_train, y_train)\n",
        "\n",
        "    rmse = model_gbr.score(X_test, y_test)\n",
        "\n",
        "    mlflow.log_metric(\"RMSE\", float(rmse))\n",
        "\n",
        "    print(\"Registering model pipeline\")\n",
        "     # Log the trained model to MLflow\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=model_gbr,\n",
        "        registered_model_name=\"gbr_airbnb_redictor\",\n",
        "        artifact_path=\"gbr_airbnb_predictor\"\n",
        "    )\n",
        "    # End the MLflow run\n",
        "\n",
        "    mlflow.end_run()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./src/main.py\n"
        }
      ],
      "execution_count": 115,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "**The inputs parameter specifies the input data **\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary modules\n",
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input\n",
        "\n",
        "# Define a new AML job using the `command` function\n",
        "job = command(\n",
        "    inputs=dict(\n",
        "        data=Input(\n",
        "            type=\"uri_file\",\n",
        "            path=\"azureml:airbnb_data:1\", # The path to the input data file\n",
        "        )),\n",
        "    # Specify the directory containing the code to be run in the job\n",
        "    code=\"./src/\",\n",
        "    # Specify the command to be run in the job, including the input data and parameters as command line arguments\n",
        "    command=\"python main.py --data ${{inputs.data}}\",\n",
        "    # Specify the environment to be used for the job\n",
        "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
        "    # Specify the compute target to be used for the job\n",
        "    compute=\"cpu-cluster\",\n",
        "    # Specify the name of the experiment for the job\n",
        "    experiment_name=\"airbnb_exper_test\",\n",
        "     # Specify the display name for the job\n",
        "    display_name=\"airbnb_display_name\",\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": 116,
      "metadata": {
        "gather": {
          "logged": 1694792915728
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Submit the job"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ml_client.create_or_update will create a new job if it does not exist or update the existing job if it does\n",
        "ml_client.create_or_update(job)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 117,
          "data": {
            "text/plain": "Command({'parameters': {}, 'init': False, 'name': 'sweet_jelly_lwksf11m25', 'type': 'command', 'status': 'Starting', 'log_files': None, 'description': None, 'tags': {}, 'properties': {'_azureml.ComputeTargetType': 'amlctrain', 'ContentSnapshotId': 'b663a62a-ab5b-4312-94c6-401fd7ea0f7d'}, 'print_as_yaml': True, 'id': '/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourceGroups/testml/providers/Microsoft.MachineLearningServices/workspaces/azureml/jobs/sweet_jelly_lwksf11m25', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/testlab/code/Users/TESTXRC71JBY4S_1686047967735/w7_monitoring_p1', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fc7fb75f640>, 'serialize': <msrest.serialization.Serializer object at 0x7fc7fb656710>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'airbnb_display_name', 'experiment_name': 'airbnb_exper_test', 'compute': 'cpu-cluster', 'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourceGroups/testml/providers/Microsoft.MachineLearningServices/workspaces/azureml?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/sweet_jelly_lwksf11m25?wsid=/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourcegroups/testml/workspaces/azureml&tid=904e9a61-6ae1-4978-87de-569b3878366c', 'type': 'Studio'}}, 'comment': None, 'job_inputs': {'data': {'type': 'uri_file', 'path': 'airbnb_data:1', 'mode': 'ro_mount'}}, 'job_outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.sweet_jelly_lwksf11m25', 'mode': 'rw_mount'}}, 'inputs': {'data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7fc7fb6568f0>}, 'outputs': {'default': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7fc7fb656800>}, 'component': CommandComponent({'intellectual_property': None, 'auto_increment_version': True, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': False, 'auto_delete_setting': None, 'name': 'sweet_jelly_lwksf11m25', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': PosixPath('.'), 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fc7fb75f640>, 'serialize': <msrest.serialization.Serializer object at 0x7fc7fb6569e0>, 'command': 'python main.py --data ${{inputs.data}}', 'code': '/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourceGroups/testml/providers/Microsoft.MachineLearningServices/workspaces/azureml/codes/0d1d7c20-b2b2-4286-8303-58b8fd8f04e0/versions/1', 'environment_variables': {}, 'environment': 'azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest', 'distribution': None, 'resources': None, 'queue_settings': None, 'version': None, 'latest_version': None, 'schema': None, 'type': 'command', 'display_name': 'airbnb_display_name', 'is_deterministic': True, 'inputs': {'data': {'type': 'uri_file', 'path': '/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourceGroups/testml/providers/Microsoft.MachineLearningServices/workspaces/azureml/data/airbnb_data/versions/1', 'mode': 'ro_mount'}}, 'outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.sweet_jelly_lwksf11m25', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {'status': 'Starting', 'parameters': {}}, 'additional_includes': [], 'CommandComponent__additional_includes_obj': None}), 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': {'Tracking': {'endpoint': 'azureml://eastus.api.azureml.ms/mlflow/v1.0/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourceGroups/testml/providers/Microsoft.MachineLearningServices/workspaces/azureml?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/sweet_jelly_lwksf11m25?wsid=/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourcegroups/testml/workspaces/azureml&tid=904e9a61-6ae1-4978-87de-569b3878366c', 'type': 'Studio'}}, 'status': 'Starting', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7fc7fb75f640>}, 'instance_id': '9a01572f-a0c9-4dd6-b7ff-bcf17d52cd2a', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': 'azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest', 'resources': {'instance_count': 1, 'shm_size': '2g'}, 'queue_settings': None, 'swept': False})",
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>airbnb_exper_test</td><td>sweet_jelly_lwksf11m25</td><td>command</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/sweet_jelly_lwksf11m25?wsid=/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourcegroups/testml/workspaces/azureml&amp;tid=904e9a61-6ae1-4978-87de-569b3878366c\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 117,
      "metadata": {
        "gather": {
          "logged": 1694792921176
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "**Note 1: We can check the status and outcome of the training job by going to the \"Jobs\" on the left --> Metrics.**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "**Note: What happens during job execution: **\n",
        "\n",
        "As the job is executed, it goes through the following stages:\n",
        "\n",
        "* **Preparing**: A docker image is created according to the environment defined. The image is uploaded to the workspace's container registry and cached for later runs. Logs are also streamed to the run history and can be viewed to monitor progress. If a curated environment is specified, the cached image backing that curated environment will be used.\n",
        "\n",
        "* **Scaling**: The cluster attempts to scale up if the cluster requires more nodes to execute the run than are currently available.\n",
        "\n",
        "* **Running**: All scripts in the script folder src are uploaded to the compute target, data stores are mounted or copied, and the script is executed. Outputs from stdout and the ./logs folder are streamed to the run history and can be used to monitor the run.\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "### Find and register the model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml.constants import AssetTypes, InputOutputModes\n",
        "\n",
        "model_name = \"gbr_airbnb_redictor_v2\"\n",
        "\n",
        "model = Model(\n",
        "    name=model_name,\n",
        "    #The name of the MLflow model.\n",
        "    path=\"gbr_airbnb_redictor/gbr_airbnb_predictor\",\n",
        "    #Path to the root directory of the model.\n",
        "    type=AssetTypes.MLFLOW_MODEL,\n",
        "    #The type of the model asset(MLflow model).\n",
        "    description=\"MLflow model for the airbnb problem\",\n",
        "    #The purpose of the model.\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 118,
      "metadata": {
        "gather": {
          "logged": 1694793404297
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "We can then register this model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "registered_model = ml_client.models.create_or_update(model=model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\rUploading gbr_airbnb_predictor (0.22 MBs):   0%|          | 0/221411 [00:00<?, ?it/s]\rUploading gbr_airbnb_predictor (0.22 MBs):   0%|          | 718/221411 [00:00<00:34, 6459.42it/s]\rUploading gbr_airbnb_predictor (0.22 MBs): 100%|██████████| 221411/221411 [00:00<00:00, 1651976.23it/s]\n\n\n"
        }
      ],
      "execution_count": 119,
      "metadata": {
        "gather": {
          "logged": 1694793408498
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Import the required libraries"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import required libraries\n",
        "from azure.ai.ml.entities import (\n",
        "    ManagedOnlineEndpoint,\n",
        "    ManagedOnlineDeployment,\n",
        "    Model,\n",
        "    Environment,\n",
        "    CodeConfiguration,\n",
        ")\n",
        "from azure.ai.ml.constants import AssetTypes"
      ],
      "outputs": [],
      "execution_count": 120,
      "metadata": {
        "gather": {
          "logged": 1694793411607
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Create Online Endpoint\n",
        "\n",
        "Online endpoints are endpoints that are used for online (real-time) inferencing. Online endpoints contain deployments that are ready to receive data from clients and can send responses back in real time.\n",
        "\n",
        "To create an online endpoint we will use `ManagedOnlineEndpoint`. This class allows user to configure the following key aspects such as `name`,`auth_mode`,`identity`,etc.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Configure the endpoint"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the required modules\n",
        "import random\n",
        "import string\n",
        "\n",
        "# Creating a unique endpoint name by including a random suffix\n",
        "\n",
        "# Defining a list of allowed characters for the endpoint suffix\n",
        "allowed_chars = string.ascii_lowercase + string.digits\n",
        "\n",
        "# Generating a random 5-character suffix for the endpoint name by choosing\n",
        "# characters randomly from the list of allowed characters\n",
        "endpoint_suffix = \"\".join(random.choice(allowed_chars) for x in range(5))\n",
        "\n",
        "# Creating the final endpoint name by concatenating a prefix string\n",
        "# with the generated suffix string\n",
        "endpoint_name = \"airbnb-endpoint-\" + endpoint_suffix\n"
      ],
      "outputs": [],
      "execution_count": 121,
      "metadata": {
        "gather": {
          "logged": 1694793413537
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Endpoint name: {endpoint_name}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Endpoint name: airbnb-endpoint-4un1w\n"
        }
      ],
      "execution_count": 122,
      "metadata": {
        "gather": {
          "logged": 1694793414180
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "endpoint = ManagedOnlineEndpoint(\n",
        "    name=endpoint_name,  \n",
        "    # Name of the endpoint, should be unique within your deployment\n",
        "    description=\"An online endpoint serving an MLflow model for the pima classification task\",\n",
        "    # A string describing the purpose of the endpoint\n",
        "    auth_mode=\"key\",\n",
        "    # Authentication mode to use for the endpoint (in this case, using an API key)\n",
        "    tags={\"foo\": \"bar\"},\n",
        "    # A dictionary of key-value pairs that can be used to tag the endpoint\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 123,
      "metadata": {
        "gather": {
          "logged": 1694793414836
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Create the endpoint\n",
        "Using the `MLClient` created earlier, we will now create the Endpoint in the workspace. This command will start the endpoint creation and return a confirmation response while the endpoint creation continues."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 124,
          "data": {
            "text/plain": "ManagedOnlineEndpoint({'public_network_access': 'Enabled', 'provisioning_state': 'Succeeded', 'scoring_uri': 'https://airbnb-endpoint-4un1w.eastus.inference.ml.azure.com/score', 'openapi_uri': 'https://airbnb-endpoint-4un1w.eastus.inference.ml.azure.com/swagger.json', 'name': 'airbnb-endpoint-4un1w', 'description': 'An online endpoint serving an MLflow model for the pima classification task', 'tags': {'foo': 'bar'}, 'properties': {'azureml.onlineendpointid': '/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourcegroups/testml/providers/microsoft.machinelearningservices/workspaces/azureml/onlineendpoints/airbnb-endpoint-4un1w', 'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/oe:747c057e-f2cb-42a6-8a0b-069e8e89448d:f37b41ad-5633-434e-b9ea-5778d70117b6?api-version=2022-02-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourceGroups/testml/providers/Microsoft.MachineLearningServices/workspaces/azureml/onlineEndpoints/airbnb-endpoint-4un1w', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/testlab/code/Users/TESTXRC71JBY4S_1686047967735/w7_monitoring_p1', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fc7fb5a5780>, 'auth_mode': 'key', 'location': 'eastus', 'identity': <azure.ai.ml.entities._credentials.IdentityConfiguration object at 0x7fc7fb75f1f0>, 'traffic': {}, 'mirror_traffic': {}, 'kind': 'Managed'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 124,
      "metadata": {
        "gather": {
          "logged": 1694793479984
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Create a blue deployment\n",
        "\n",
        "A deployment is a set of resources required for hosting the model that does the actual inferencing. We will create a deployment for our endpoint using the `ManagedOnlineDeployment` class. This class allows user to configure key aspects."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "##### Curating the deployment script:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "**Go to the Microsoft Azure home page and search for Application Insights --> Copy the Connection String and use it in the score.py script.**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {src_dir}/score.py\n",
        "\n",
        "# Import necessary libraries and modules\n",
        "import logging\n",
        "import os\n",
        "import json\n",
        "import mlflow\n",
        "from io import StringIO\n",
        "from mlflow.pyfunc.scoring_server import infer_and_parse_json_input, predictions_to_json\n",
        "\n",
        "######################LOGGER#####################\n",
        "# Set up Azure logging\n",
        "import logging\n",
        "from logging import Logger\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "\n",
        "# Connect to Application Insights and set logging level to INFO\n",
        "application_insights_connection_string= 'InstrumentationKey=e040f1cb-4a7a-4009-87be-7861c813da51;IngestionEndpoint=https://eastus-8.in.applicationinsights.azure.com/;LiveEndpoint=https://eastus.livediagnostics.monitor.azure.com/'\n",
        "handler = AzureLogHandler(\n",
        "connection_string=application_insights_connection_string)\n",
        "logger = logging.getLogger()\n",
        "logger.addHandler(handler)\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "####################################################\n",
        "\n",
        "# Define the init() function to load the MLflow model\n",
        "def init():\n",
        "    global model\n",
        "    global input_schema\n",
        "    # \"model\" is the path of the mlflow artifacts when the model was registered. For automl\n",
        "    # models, this is generally \"mlflow-model\".\n",
        "    model_path = os.path.join(os.getenv(\"AZUREML_MODEL_DIR\"), \"gbr_airbnb_predictor\")\n",
        "    model = mlflow.pyfunc.load_model(model_path)\n",
        "    input_schema = model.metadata.get_input_schema()\n",
        "\n",
        "# Define the run() function to make predictions using the loaded model\n",
        "def run(raw_data):\n",
        "    # Parse input data\n",
        "    json_data = json.loads(raw_data)\n",
        "    if \"input_data\" not in json_data.keys():\n",
        "        raise Exception(\"Request must contain a top level key named 'input_data'\")\n",
        "    serving_input = json.dumps(json_data[\"input_data\"])\n",
        "    data = infer_and_parse_json_input(serving_input, input_schema)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(data)\n",
        "\n",
        "    # Log the input data and predictions to Azure\n",
        "    logger.info(\"Data:{0},Predictions:{1}\".format(str(data),str(predictions)))\n",
        "\n",
        "    # Convert predictions to JSON format and return\n",
        "    result = StringIO()\n",
        "    predictions_to_json(predictions, result)\n",
        "    return result.getvalue()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./src/score.py\n"
        }
      ],
      "execution_count": 125,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "**Curating the deployment:**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new deployment with name \"blue\"\n",
        "blue_deployment = ManagedOnlineDeployment(\n",
        "    name=\"blue\",\n",
        "    # Use the previously generated endpoint name\n",
        "    endpoint_name=endpoint_name,\n",
        "    # Use the registered model\n",
        "    model=registered_model,\n",
        "    # Use the latest environment\n",
        "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
        "    # Use the code in the \"./src\" directory and the \"score.py\" script\n",
        "    code_configuration=CodeConfiguration(\n",
        "        code=\"./src\", scoring_script=\"score.py\"\n",
        "    ),\n",
        "    # Use a single instance of type \"Standard_E2s_v3\"\n",
        "    instance_type=\"Standard_E2s_v3\",\n",
        "    instance_count=1,\n",
        "    # Enable Application Insights for the deployment\n",
        "    app_insights_enabled=True,\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 126,
      "metadata": {
        "gather": {
          "logged": 1694793520289
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "source": [
        "#### Create the deployment\n",
        "\n",
        "Using the `MLClient` created earlier, we will now create the deployment in the workspace. This command will start the deployment creation and return a confirmation response while the deployment creation continues."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Check: endpoint airbnb-endpoint-4un1w exists\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "................................................................................................................"
        },
        {
          "output_type": "execute_result",
          "execution_count": 127,
          "data": {
            "text/plain": "ManagedOnlineDeployment({'private_network_connection': None, 'provisioning_state': 'Succeeded', 'endpoint_name': 'airbnb-endpoint-4un1w', 'type': 'Managed', 'name': 'blue', 'description': None, 'tags': {}, 'properties': {'AzureAsyncOperationUri': 'https://management.azure.com/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/providers/Microsoft.MachineLearningServices/locations/eastus/mfeOperationsStatus/od:747c057e-f2cb-42a6-8a0b-069e8e89448d:fab0816e-da5f-4713-9488-b16456016a81?api-version=2023-04-01-preview'}, 'print_as_yaml': True, 'id': '/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourceGroups/testml/providers/Microsoft.MachineLearningServices/workspaces/azureml/onlineEndpoints/airbnb-endpoint-4un1w/deployments/blue', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/testlab/code/Users/TESTXRC71JBY4S_1686047967735/w7_monitoring_p1', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7fc7fb5a6800>, 'model': '/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourceGroups/testml/providers/Microsoft.MachineLearningServices/workspaces/azureml/models/gbr_airbnb_redictor_v2/versions/3', 'code_configuration': {'code': '/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourceGroups/testml/providers/Microsoft.MachineLearningServices/workspaces/azureml/codes/6a9dab7e-c97c-4b06-bb8c-2ca012d30b37/versions/1'}, 'environment': '/subscriptions/1c6d9ef7-1867-436f-986f-f37c475a295b/resourceGroups/testml/providers/Microsoft.MachineLearningServices/workspaces/azureml/environments/AzureML-sklearn-1.0-ubuntu20.04-py38-cpu/versions/36', 'environment_variables': {}, 'app_insights_enabled': True, 'scale_settings': <azure.ai.ml.entities._deployment.scale_settings.DefaultScaleSettings object at 0x7fc7fb5a5570>, 'request_settings': <azure.ai.ml.entities._deployment.deployment_settings.OnlineRequestSettings object at 0x7fc7fb5a6d10>, 'liveness_probe': <azure.ai.ml.entities._deployment.deployment_settings.ProbeSettings object at 0x7fc7fb5a5720>, 'readiness_probe': <azure.ai.ml.entities._deployment.deployment_settings.ProbeSettings object at 0x7fc7fb5a7850>, 'instance_count': 1, 'arm_type': 'online_deployment', 'model_mount_path': None, 'instance_type': 'Standard_E2s_v3', 'data_collector': None, 'egress_public_network_access': 'Enabled'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 127,
      "metadata": {
        "gather": {
          "logged": 1694794106898
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Directional Expectation"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This task involves monitoring the alignment of model predictions with the expected direction of change. In many scenarios, organizations have a predefined understanding of how certain variables should impact the outcome.  Monitoring the directional expectation involves analyzing whether the model's predictions align with these predefined expectations. By comparing the model's predictions with the anticipated direction of change, organizations can quickly identify potential issues or anomalies that require further investigation. This task allows for an initial assessment of the model's performance and can provide insights into any unexpected behaviors or deviations from the expected patterns."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=endpoint_name,\n",
        "        deployment_name=\"blue\",\n",
        "        request_file=\"baseline-data.json\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 129,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694795015202
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\"{\\\"predictions\\\": [5.02775838674696]}\"\n"
        }
      ],
      "execution_count": 130,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694795035543
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=endpoint_name,\n",
        "        deployment_name=\"blue\",\n",
        "        request_file=\"test-data.json\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 132,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694795162795
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\"{\\\"predictions\\\": [4.917771115184388]}\"\n"
        }
      ],
      "execution_count": 133,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694795168423
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perturbation Analysis"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perturbation analysis involves introducing deliberate changes or perturbations to the input data and observing the corresponding impact on model predictions. This task helps evaluate the stability and robustness of the model. By systematically perturbing variables or introducing simulated variations, organizations can assess how sensitive the model is to different inputs and determine if it responds in an expected manner. For instance, in a credit risk assessment model, perturbation analysis could involve altering individual features such as income or credit utilization ratios to observe how the model's predictions change. This analysis helps identify potential vulnerabilities or inconsistencies in the model's behavior and informs the need for recalibration or retraining."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=endpoint_name,\n",
        "        deployment_name=\"blue\",\n",
        "        request_file=\"baseline-payload-data.json\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 136,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694795374586
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\"{\\\"predictions\\\": [4.988102857005535]}\"\n"
        }
      ],
      "execution_count": 137,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694795385486
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=endpoint_name,\n",
        "        deployment_name=\"blue\",\n",
        "        request_file=\"test-payload-data.json\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 138,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694795401226
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\"{\\\"predictions\\\": [5.037330794542146]}\"\n"
        }
      ],
      "execution_count": 139,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694795411392
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitoring Critical Subgroups"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In some applications, it is important to monitor the performance of machine learning models specifically for critical subgroups or segments of the population. These subgroups may be defined by demographic characteristics, geographic location, or other relevant factors. For example, in healthcare, it is crucial to ensure that a medical diagnosis model performs well across different demographic groups to avoid bias or disparities in patient care. Short-term monitoring tasks involve regularly assessing model performance metrics, such as accuracy or false positive rates, specifically for these critical subgroups. If significant disparities or performance gaps are detected, further investigation can be conducted to understand the root causes and take necessary corrective actions, such as retraining the model or adjusting its decision thresholds."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = ml_client.online_endpoints.invoke(\n",
        "    endpoint_name=endpoint_name,\n",
        "        deployment_name=\"blue\",\n",
        "        request_file=\"critical-sku-payload-data.json\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 143,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694795553023
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\"{\\\"predictions\\\": [6.1218026828745975]}\"\n"
        }
      ],
      "execution_count": 144,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1694795554282
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}