{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide comprehensive understanding of MLOps, including an overview of the concept, the importance of model pipelines in cloud platforms such as AWS and Azure, and the integration of version control using Git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DevOps -> MLOps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![devops](assets/devops-cycle.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.hiclipart.com/free-transparent-background-png-clipart-pytym)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DevOps cycle encompasses a set of steps that facilitate the development, deployment, and operation of software systems. In the context of machine learning, these steps can be adapted to create an ML DevOps cycle, which focuses on managing and automating the machine learning workflow. Here's an explanation of each step in the ML DevOps cycle, along with an example for each stage:\n",
    "\n",
    "1. Code:\n",
    "   - In the ML DevOps cycle, the \"Code\" stage involves developing and maintaining the machine learning codebase. This includes writing code for data preprocessing, model training, evaluation, and deployment.\n",
    "   - Example: A data scientist writes Python scripts to preprocess data, build and train machine learning models, and evaluate their performance.\n",
    "\n",
    "2. Build:\n",
    "   - The \"Build\" stage involves packaging the code and its dependencies into a deployable format. This step ensures that the ML codebase can be easily reproduced and deployed in different environments.\n",
    "   - Example: Using tools like Docker, the ML codebase is containerized, creating a portable and self-contained package that encapsulates the required dependencies and configurations.\n",
    "\n",
    "3. Test:\n",
    "   - The \"Test\" stage focuses on verifying the functionality, quality, and performance of the ML codebase. This includes running unit tests, integration tests, and evaluating model performance on test datasets.\n",
    "   - Example: Unit tests are written to verify the correctness of individual functions or modules in the ML codebase, while integration tests assess the interoperability of different components.\n",
    "\n",
    "4. Release:\n",
    "   - The \"Release\" stage involves preparing the ML codebase for deployment in a production environment. This includes generating deployment artifacts, documenting release notes, and ensuring that all necessary dependencies are included.\n",
    "   - Example: A trained machine learning model is packaged along with the required preprocessing scripts, trained weights, and configuration files for seamless deployment.\n",
    "\n",
    "5. Deploy:\n",
    "   - In the \"Deploy\" stage, the ML codebase is deployed to a production environment, making it available for serving predictions or integrating with other systems.\n",
    "   - Example: The packaged ML codebase is deployed to a cloud-based server or an edge device, allowing real-time predictions to be made based on incoming data.\n",
    "\n",
    "6. Operate:\n",
    "   - The \"Operate\" stage focuses on monitoring and managing the deployed ML system. This includes logging relevant metrics, handling errors, and ensuring the system's health and availability.\n",
    "   - Example: A monitoring system is set up to track the prediction latency, accuracy, and other relevant metrics of the deployed ML model. Alerts are generated if the system's performance deviates from defined thresholds.\n",
    "\n",
    "7. Monitor:\n",
    "   - The \"Monitor\" stage involves continuously monitoring the ML system's performance, data quality, and model behavior in a production environment. This helps identify anomalies, detect drift, and ensure ongoing reliability.\n",
    "   - Example: A monitoring pipeline periodically collects real-time data and evaluates the model's performance over time. Anomaly detection algorithms are applied to identify unexpected behavior or data drift.\n",
    "\n",
    "8. Plan:\n",
    "   - The \"Plan\" stage focuses on gathering feedback, analyzing performance, and incorporating improvements into future iterations of the ML system. It involves planning for enhancements, bug fixes, and updates.\n",
    "   - Example: Based on feedback from users or performance metrics, the ML team identifies areas for improvement, such as collecting additional data, retraining the model with new algorithms, or optimizing specific parts of the pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Integration/Continuous Deployment (CI/CD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CI/CD (Continuous Integration/Continuous Deployment) is a crucial component of the DevOps cycle in the context of machine learning. It focuses on automating the process of integrating code changes, testing, and deploying ML models, ensuring a streamlined and efficient workflow. Here's a brief summary of CI/CD in the context of the DevOps cycle for machine learning:\n",
    "\n",
    "Continuous Integration (CI):\n",
    "- CI involves automating the integration of code changes made by different developers into a shared repository. It aims to prevent integration conflicts and maintain code quality.\n",
    "- In the ML context, CI ensures that changes to the ML codebase, including data preprocessing, model training, and evaluation, are automatically integrated into a central repository.\n",
    "- CI systems, such as AWS CodeBuild, Azure DevOps or Jenkins, trigger automated builds and tests whenever changes are pushed to the repository.\n",
    "- Continuous integration facilitates collaboration, identifies integration issues early, and promotes a consistent and stable codebase.\n",
    "\n",
    "Continuous Deployment (CD):\n",
    "- CD extends CI by automating the deployment process, allowing ML models and related components to be deployed to production environments in a reliable and reproducible manner.\n",
    "- CD enables the automated execution of the machine learning pipeline, including model training, evaluation, packaging, and deployment.\n",
    "- CD systems leverage CI artifacts and trigger deployment processes based on predefined criteria, such as passing tests or specific branch merges.\n",
    "- CD ensures that ML models are consistently deployed to production, reducing manual effort and minimizing the risk of human error.\n",
    "- It enables faster and more frequent deployments, enabling rapid iteration and quicker delivery of ML-based applications.\n",
    "\n",
    "CI/CD for machine learning encompasses the integration, testing, and deployment of ML code changes, resulting in a streamlined and automated pipeline. By leveraging CI/CD practices, ML teams can achieve greater efficiency, collaboration, and reliability in developing and deploying machine learning models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key steps in CI/CD for MLOps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An operational CI/CD MLOps system hinges on two key steps:\n",
    "\n",
    "1. Pipeline Delineation:\n",
    "   A machine learning pipeline defines the sequence of steps required to train, evaluate, and deploy machine learning models. It encompasses various stages, such as data preprocessing, feature engineering, model training, validation, testing, and deployment. The pipeline ensures consistency and reproducibility in the machine learning workflow.\n",
    "\n",
    "   The pipeline can be represented as a series of interconnected components or modules, each responsible for a specific task. These components can be implemented as scripts, functions, or Docker containers. The pipeline delineation specifies the order of execution and the dependencies between the components.\n",
    "\n",
    "2. Version Control System for Tracking Pipeline Changes:\n",
    "   In an MLOps workflow, it is crucial to track changes made to the machine learning pipeline, including modifications to code, configurations, and dependencies. A version control system, such as Git, serves as the foundation for tracking and managing these changes.\n",
    "\n",
    "   With a version control system, each change made to the pipeline is recorded as a commit, capturing the specific modifications made at a given point in time. This enables traceability, reproducibility, and collaboration among team members.\n",
    "\n",
    "   When a version control system detects a change in the pipeline, it triggers the execution of the pipeline by default. This ensures that any modifications to the pipeline automatically initiate the necessary steps for retraining, reevaluation, or redeployment.\n",
    "\n",
    "   For example, if a developer makes changes to the preprocessing component of the pipeline, such as adding new data transformations or modifying existing ones, the version control system registers the changes. As a result, the pipeline execution is triggered, rerunning the preprocessing step with the updated logic.\n",
    "\n",
    "   Similarly, if a change is made to the model training component, such as using a different algorithm or adjusting hyperparameters, the version control system captures the modifications and initiates the retraining process.\n",
    "\n",
    "   By coupling the version control system with automatic pipeline execution, the MLOps system ensures that any changes to the pipeline are automatically incorporated into the workflow, reducing the manual effort required for execution and promoting reproducibility across different stages of the machine learning lifecycle."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now look at each of these components in detail."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelines are a series of steps that are connected with each other; each step accomplishes a specific machine learning task.\n",
    "\n",
    "For example, a typical machine learning pipeline might include the following steps:\n",
    "- Data ingestion and preprocessing: Cleansing, transformation, and feature engineering on raw data.\n",
    "- Model training: Training machine learning models using a specific algorithm and hyperparameters.\n",
    "- Model evaluation: Assessing the model's performance using appropriate metrics and validation techniques.\n",
    "- Model deployment: Deploying the trained model for inference or integration into a production environment."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ml-pipeline-example](assets/pipeline.drawio.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SageMaker Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "from sagemaker.inputs import TrainingInput"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker imports now include those that will enable us to assemple pipelines as a series of steps (e.g., `ProcessingStep` and `TrainingStep`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Azure ML imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "from azure.ai.ml import command, Input, Output, dsl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new import here is `dsl` which implements a domain-specific language to provide the functionality to define and build pipelines using the Azure ML Pipelines framework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SageMaker Authentication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_session = sagemaker.Session(\n",
    "    default_bucket=\"sagemaker-pipeline-examples\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    aws_role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    print('Local configuration is not complete; use SageMaker Studio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS execution role associated with the account arn:aws:iam::321112151583:role/default-sagemaker-access\n",
      "Default bucket associated with the account: sagemaker-pipeline-examples\n",
      "Default boto region associated with the account: ap-south-1\n"
     ]
    }
   ],
   "source": [
    "print(f\"AWS execution role associated with the account {aws_role}\")\n",
    "print(f\"Default bucket associated with the account: {pipeline_session.default_bucket()}\")\n",
    "print(f\"Default boto region associated with the account: {pipeline_session.boto_region_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Azure authentication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"5bcad9c4-40fb-4136-b614-cc90116dd8b3\"\n",
    "resource_group = \"tf\"\n",
    "workspace = \"cloud-teach\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could define Azure to not log verbose configuration messages by specifying the log settings to only include warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"azure.core.pipeline.policies.http_logging_policy\")\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_credentials = DefaultAzureCredential(\n",
    "    exclude_interactive_browser_credential=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client = MLClient(\n",
    "    az_credentials, subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winequality-local\n",
      "winequality-red\n",
      "user-likes-media\n",
      "socialmediaengagement\n",
      "imdb_reviews\n",
      "diamond-prices-jan\n",
      "diamond-prices-feb\n",
      "wine-quality-indicator\n",
      "diamond-prices-may\n"
     ]
    }
   ],
   "source": [
    "for registered_data in ml_client.data.list():\n",
    "    print(registered_data.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Pipelines is a powerful feature of Amazon SageMaker that allows you to create, orchestrate, and automate end-to-end machine learning workflows. It provides a high-level abstraction for defining and managing interconnected steps within a pipeline and automating the handoff between these steps using SageMaker abstractions. Here's an overview of how SageMaker Pipelines works:\n",
    "\n",
    "1. Step Definition:\n",
    "   - Each step in the pipeline represents a unit of work, such as data preprocessing, model training, or model deployment.\n",
    "   - You define each step as a reusable component using SageMaker Step Functions.\n",
    "   - The step definition includes the specific actions, algorithms, configurations, and inputs/outputs required for that particular step.\n",
    "\n",
    "2. Pipeline Definition:\n",
    "   - You define the pipeline as a series of interconnected steps using the SageMaker Pipelines SDK.\n",
    "   - The pipeline definition specifies the sequence of steps, the input/output dependencies between them, and any conditions or branching logic.\n",
    "   - You can parameterize the pipeline to make it configurable and flexible, allowing for easy experimentation with different settings.\n",
    "\n",
    "3. Data Flow and Handoff:\n",
    "   - SageMaker Pipelines automatically manages the flow of data between steps in the pipeline.\n",
    "   - Each step consumes the output produced by its preceding steps and produces outputs that can be consumed by subsequent steps.\n",
    "   - The data handoff between steps is automated by SageMaker, ensuring the seamless transfer of data and artifacts without manual intervention.\n",
    "\n",
    "4. Execution and Monitoring:\n",
    "   - Once the pipeline is defined, you can execute it using the SageMaker Pipelines SDK or the SageMaker Management Console.\n",
    "   - During pipeline execution, SageMaker handles the provisioning and management of the underlying resources required for each step.\n",
    "   - You can monitor the pipeline's progress, track metrics, and log intermediate outputs using Amazon CloudWatch and Amazon S3.\n",
    "\n",
    "5. Reusability and Versioning:\n",
    "   - SageMaker Pipelines encourage reusability and modularity by allowing you to create and reuse step definitions across multiple pipelines.\n",
    "   - You can version the step definitions and pipelines, making it easier to track changes, rollback to previous versions, and ensure reproducibility.\n",
    "\n",
    "By leveraging SageMaker Pipelines, you can build complex and scalable machine learning workflows that seamlessly integrate data preprocessing, model training, evaluation, deployment, and monitoring. The automated handoff between steps and the use of SageMaker abstractions simplifies the pipeline creation and management process, enabling efficient and reliable end-to-end machine learning workflows in Amazon SageMaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how SageMaker Pipelines work, let us assemble and execute a three-step pipeline that processes raw data, trains a model on the processed data and computes evaluation metrics using the test data and the estimated model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![aws-processing-map](assets/aws-processing-job.drawio.png) ![aws-processing-map](assets/aws-processing-map.drawio.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_uri = 's3://sagemaker-ap-south-1-321112151583/prices/diamond-prices.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"1.0-1\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"sklearn-diamond-prices-process\",\n",
    "    role=aws_role,\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_process = ProcessingStep(\n",
    "    name=\"DiamondsProcess\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "      ProcessingInput(source=input_data_uri, destination=\"/opt/ml/processing/input\"),  \n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\")\n",
    "    ],\n",
    "    code=\"aws/transform.py\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key difference from before is the usage of a `ProcessingOutput` class as the output of the step. This class will store the persistent location of the processing output as one of its attributes. The processing script `transform.py` is exactly the same as before.\n",
    "\n",
    "At this stage the processing step is registered but not executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<sagemaker.processing.ProcessingInput at 0x7f5057d7c640>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_process.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<sagemaker.processing.ProcessingOutput at 0x7f5058126850>,\n",
       " <sagemaker.processing.ProcessingOutput at 0x7f50581267c0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_process.outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![training-job](assets/aws-training-job.drawio.png) ![training-map](assets/aws-training-data-map.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"aws/dt.py\",\n",
    "    framework_version=\"1.0-1\",\n",
    "    role=aws_role,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    volume_size=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did before, we instantiate an `Estimator` that assembles the compute environment, the compute infrastructure and the training script that will be run (the training script `dt.py` is unchanged from before)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this estimator in place, we can now create the training step of the pipeline by linking the outputs of the processing step (`step_process`) to the inputs of the training step (this is the hand-off automation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train = TrainingStep(\n",
    "    name=\"DiamondsTrain\",\n",
    "    estimator=sklearn_estimator,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        ),\n",
    "        \"test\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\"\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.workflow.properties.Properties at 0x7f505828ddc0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The role of the `ProcessingOutput` class is crucial in defining the output data generated by the processing step. It allows you to specify the name of the output (output_name) and the source path within the processing container (source) where the processed data will be saved. These output artifacts can be utilized by subsequent steps in the pipeline, enabling the flow of data and dependencies between the pipeline components. This property will be dynamically updated when the pipeline runs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we will need to evaluate it using an evaluation script (`evaluate.py`). This script generates predictions from the model saved in the previous step and logs metrics on test data. This evaluation is often used to conditionally trigger next steps (e.g., if the accuracy is not greater than the current baseline do not trigger further execution). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SageMaker pipelines, evaluation metric computation is considered a processing step because it involves performing some data processing or analysis to generate evaluation metrics for a machine learning model. This step is typically implemented using a `ScriptProcessor` and a `PropertyFile` in SageMaker pipelines. \n",
    "\n",
    "1. ScriptProcessor:\n",
    "   - The `ScriptProcessor` is a SageMaker feature that enables the execution of custom scripts as part of a processing step.\n",
    "   - In the case of evaluation metric computation, a custom script is used to perform the necessary calculations or analysis to derive the desired metrics.\n",
    "\n",
    "2. PropertyFile:\n",
    "   - A `PropertyFile` is often used in conjunction with the `ScriptProcessor` to capture and store the evaluation metrics generated by the custom script.\n",
    "   - The custom script writes the evaluation metrics to a property file, which is then registered as an output of the processing step.\n",
    "   - This allows the evaluation metrics to be captured and passed to subsequent steps in the pipeline for further processing or analysis.\n",
    "\n",
    "In this way, we can:\n",
    "\n",
    "- Maintain a modular and reusable pipeline structure, where each step has a well-defined purpose and encapsulates specific processing or analysis tasks.\n",
    "- Easily track and manage the flow of data and metrics within the pipeline, enabling seamless integration with other pipeline components.\n",
    "- Leverage the flexibility and scalability of SageMaker's processing capabilities, such as distributed processing, managed infrastructure, and resource provisioning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the evaluation step, we need to assemble the execution environment and hand it over to the `ScriptProcessor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "720646828776.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3\n"
     ]
    }
   ],
   "source": [
    "session_region = 'ap-south-1'  # Replace with your desired region\n",
    "\n",
    "# Get the specific SKLearn image URI for the given region\n",
    "sklearn_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='sklearn',\n",
    "    version='1.0-1',\n",
    "    region=session_region\n",
    ")\n",
    "\n",
    "print(sklearn_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_eval = ScriptProcessor(\n",
    "    image_uri=sklearn_image_uri,\n",
    "    command=[\"python\"],\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-diamonds-eval\",\n",
    "    role=aws_role,\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metric collection within a SageMaker pipeline should follow a [specific format](https://docs.aws.amazon.com/sagemaker/latest/dg/model-monitor-model-quality-metrics.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_eval = ProcessingStep(\n",
    "    name=\"DiamondsEval\",\n",
    "    processor=script_eval,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test\"\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"aws/evaluate.py\",\n",
    "    property_files=[evaluation_report]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the input and output channels are grabbing connections from the previous step. \n",
    "\n",
    "1. `inputs=[ProcessingInput(...), ProcessingInput(...)]`:\n",
    "\n",
    "The inputs parameter is a list that defines the input data required for the evaluation step.\n",
    "The first `ProcessingInput` specifies the source location of the trained model artifacts (`step_train.properties.ModelArtifacts.S3ModelArtifacts`) and sets the destination path within the processing container as `/opt/ml/processing/model`. This provides the model artifacts to the evaluation script.\n",
    "The second `ProcessingInput` specifies the source location of the processed test data output from a previous step (`step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri`) and sets the destination path within the processing container as `/opt/ml/processing/test`. This provides the test data to the evaluation script.\n",
    "\n",
    "2. `outputs=[ProcessingOutput(...)]`:\n",
    "\n",
    "The `outputs` parameter is a list that defines the output generated by the evaluation step.\n",
    "The `ProcessingOutput` specifies the output name as `evaluation` and sets the source path within the processing container as `/opt/ml/processing/evaluation`. This defines where the evaluation results will be stored. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Assembling the pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the key components of a training pipeline that processes data, trains a model on the processed data and logs metrics from the data. It is now time to assemble the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=\"DiamondsPipeline\",\n",
    "    steps=[step_process, step_train, step_eval],\n",
    "    sagemaker_session=pipeline_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Version': '2020-12-01',\n",
       " 'Metadata': {},\n",
       " 'Parameters': [],\n",
       " 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'},\n",
       "  'TrialName': {'Get': 'Execution.PipelineExecutionId'}},\n",
       " 'Steps': [{'Name': 'DiamondsProcess',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '720646828776.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python3',\n",
       "      '/opt/ml/processing/input/code/transform.py']},\n",
       "    'RoleArn': 'arn:aws:iam::321112151583:role/default-sagemaker-access',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-ap-south-1-321112151583/prices/diamond-prices.csv',\n",
       "       'LocalPath': '/opt/ml/processing/input',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-pipeline-examples/DiamondsProcess-af97f4591fb975d3dcb7c73700b81fce/input/code/transform.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-pipeline-examples',\n",
       "           'DiamondsPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'DiamondsProcess',\n",
       "           'output',\n",
       "           'train']}},\n",
       "        'LocalPath': '/opt/ml/processing/train',\n",
       "        'S3UploadMode': 'EndOfJob'}},\n",
       "      {'OutputName': 'test',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-pipeline-examples',\n",
       "           'DiamondsPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'DiamondsProcess',\n",
       "           'output',\n",
       "           'test']}},\n",
       "        'LocalPath': '/opt/ml/processing/test',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}}},\n",
       "  {'Name': 'DiamondsTrain',\n",
       "   'Type': 'Training',\n",
       "   'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File',\n",
       "     'TrainingImage': '720646828776.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3'},\n",
       "    'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-pipeline-examples/'},\n",
       "    'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       "    'ResourceConfig': {'VolumeSizeInGB': 1,\n",
       "     'InstanceCount': 1,\n",
       "     'InstanceType': 'ml.m5.xlarge'},\n",
       "    'RoleArn': 'arn:aws:iam::321112151583:role/default-sagemaker-access',\n",
       "    'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.DiamondsProcess.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'train'},\n",
       "     {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix',\n",
       "        'S3Uri': {'Get': \"Steps.DiamondsProcess.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "        'S3DataDistributionType': 'FullyReplicated'}},\n",
       "      'ContentType': 'text/csv',\n",
       "      'ChannelName': 'test'}],\n",
       "    'HyperParameters': {'sagemaker_submit_directory': '\"s3://sagemaker-pipeline-examples/DiamondsTrain-ea10d3d4443fdf099fa604049ba86d50/source/sourcedir.tar.gz\"',\n",
       "     'sagemaker_program': '\"dt.py\"',\n",
       "     'sagemaker_container_log_level': '20',\n",
       "     'sagemaker_region': '\"ap-south-1\"'},\n",
       "    'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-pipeline-examples/',\n",
       "     'CollectionConfigurations': []},\n",
       "    'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-pipeline-examples/',\n",
       "     'DisableProfiler': False}}},\n",
       "  {'Name': 'DiamondsEval',\n",
       "   'Type': 'Processing',\n",
       "   'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge',\n",
       "      'InstanceCount': 1,\n",
       "      'VolumeSizeInGB': 30}},\n",
       "    'AppSpecification': {'ImageUri': '720646828776.dkr.ecr.ap-south-1.amazonaws.com/sagemaker-scikit-learn:1.0-1-cpu-py3',\n",
       "     'ContainerEntrypoint': ['python',\n",
       "      '/opt/ml/processing/input/code/evaluate.py']},\n",
       "    'RoleArn': 'arn:aws:iam::321112151583:role/default-sagemaker-access',\n",
       "    'ProcessingInputs': [{'InputName': 'input-1',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': 'Steps.DiamondsTrain.ModelArtifacts.S3ModelArtifacts'},\n",
       "       'LocalPath': '/opt/ml/processing/model',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'input-2',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': {'Get': \"Steps.DiamondsProcess.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"},\n",
       "       'LocalPath': '/opt/ml/processing/test',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}},\n",
       "     {'InputName': 'code',\n",
       "      'AppManaged': False,\n",
       "      'S3Input': {'S3Uri': 's3://sagemaker-pipeline-examples/DiamondsEval-8af21ff5c9f0642816519169a0b44791/input/code/evaluate.py',\n",
       "       'LocalPath': '/opt/ml/processing/input/code',\n",
       "       'S3DataType': 'S3Prefix',\n",
       "       'S3InputMode': 'File',\n",
       "       'S3DataDistributionType': 'FullyReplicated',\n",
       "       'S3CompressionType': 'None'}}],\n",
       "    'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'evaluation',\n",
       "       'AppManaged': False,\n",
       "       'S3Output': {'S3Uri': {'Std:Join': {'On': '/',\n",
       "          'Values': ['s3:/',\n",
       "           'sagemaker-pipeline-examples',\n",
       "           'DiamondsPipeline',\n",
       "           {'Get': 'Execution.PipelineExecutionId'},\n",
       "           'DiamondsEval',\n",
       "           'output',\n",
       "           'evaluation']}},\n",
       "        'LocalPath': '/opt/ml/processing/evaluation',\n",
       "        'S3UploadMode': 'EndOfJob'}}]}},\n",
       "   'PropertyFiles': [{'PropertyFileName': 'EvaluationReport',\n",
       "     'OutputName': 'evaluation',\n",
       "     'FilePath': 'evaluation.json'}]}]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(pipeline.definition())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once a pipeline is registered, it needs to be \"upserted\" (= update + insert) on the SageMaker infrastructure. The purpose of `pipeline.upsert(role_arn=aws_role)` is to create or update the pipeline in the AWS infrastructure, ensuring that the specified role has the necessary permissions to execute the pipeline. This operation enables us to deploy and manage the pipeline definition within our AWS environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n",
      "Using provided s3_resource\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:ap-south-1:321112151583:pipeline/diamondspipeline',\n",
       " 'ResponseMetadata': {'RequestId': '909d8e6e-c938-4e28-b6fe-2c55ab3592fa',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '909d8e6e-c938-4e28-b6fe-2c55ab3592fa',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '85',\n",
       "   'date': 'Tue, 06 Jun 2023 12:39:07 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=aws_role)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is now ready for execution (`HTTPStatusCode` is 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The status of execution can be monitored using the `describe()` method of the execution object or in the SageMaker Studio UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:ap-south-1:321112151583:pipeline/diamondspipeline',\n",
       " 'PipelineExecutionArn': 'arn:aws:sagemaker:ap-south-1:321112151583:pipeline/diamondspipeline/execution/94zb3pycywap',\n",
       " 'PipelineExecutionDisplayName': 'execution-1686055153748',\n",
       " 'PipelineExecutionStatus': 'Executing',\n",
       " 'PipelineExperimentConfig': {'ExperimentName': 'diamondspipeline',\n",
       "  'TrialName': '94zb3pycywap'},\n",
       " 'CreationTime': datetime.datetime(2023, 6, 6, 18, 9, 13, 630000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2023, 6, 6, 18, 9, 13, 630000, tzinfo=tzlocal()),\n",
       " 'CreatedBy': {},\n",
       " 'LastModifiedBy': {},\n",
       " 'ResponseMetadata': {'RequestId': '5ebf94a1-59b7-4cb3-9ad1-cadafdae4009',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '5ebf94a1-59b7-4cb3-9ad1-cadafdae4009',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '489',\n",
       "   'date': 'Tue, 06 Jun 2023 12:40:13 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure ML pipelines are a way to define and orchestrate a series of interconnected steps or tasks to automate end-to-end machine learning workflows. These pipelines provide a structured and scalable approach to building, deploying, and managing machine learning workflows using Azure ML abstractions. Here's an explanation of Azure ML pipelines:\n",
    "\n",
    "1. Components:\n",
    "   - An Azure ML pipeline consists of multiple steps (called components), where each step represents a specific task or operation within the workflow.\n",
    "   - Each step can include activities such as data preparation, feature engineering, model training, model evaluation, deployment, and more.\n",
    "   - Steps can be connected in a sequential manner, where the output of one step serves as the input for the next step, allowing for a seamless flow of data and execution.\n",
    "\n",
    "2. Interconnected Workflow:\n",
    "   - Azure ML pipelines enable the creation of a workflow where the steps are interconnected, allowing for the automatic flow of data and dependencies between the steps.\n",
    "   - By defining the dependencies between steps, Azure ML ensures that each step is executed in the correct order, respecting the dependencies and data flow.\n",
    "\n",
    "3. Handoff Automation:\n",
    "   - Azure ML pipelines automate the handoff between steps by managing the input and output data of each step.\n",
    "   - The output of one step is automatically passed as input to the subsequent step, eliminating the need for manual intervention or data transfer.\n",
    "   - This automation simplifies the overall workflow and reduces the risk of errors or inconsistencies in data handoff."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![azure-processing-map](assets/azure-processing-map.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "diamond_prices = ml_client.data.get(\"diamond-prices-jan\", version=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_process = command(\n",
    "    name=\"data_prep_diamond_prices\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"read a .csv input, split the input to train and test\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_folder\"),\n",
    "        \"test_train_ratio\": Input(type=\"number\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code='azure/components/data_prep/',\n",
    "    command=\"\"\"python tts.py \\\n",
    "            --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
    "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
    "            \"\"\",\n",
    "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key difference from before is the usage of the `Output` class to capture the output from the processing script. This object captures the output as a registered folder and holds the state as an attribute. This will enable us to automate the handoff between the processing step and the training step.\n",
    "\n",
    "Previously this command could itself be executed using `ml_client.create_or_update` method. However, we are going to hold off execution and treat each command as a component. So, while the command is aware of the input types and the output types, it cannot be executed unless specific instances of these are realized as a part of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'type': 'uri_folder'}, 'test_train_ratio': {'type': 'number'}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_process.component.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_data': {'type': 'uri_folder', 'mode': 'rw_mount'},\n",
       " 'test_data': {'type': 'uri_folder', 'mode': 'rw_mount'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_process.component.outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![azure-train-map](assets/azure-training-map.drawio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_train = command(\n",
    "    name=\"train_diamond_prices_model\",\n",
    "    display_name=\"Training a diamond price model\",\n",
    "    description=\"train a gradient boosted regression model\",\n",
    "    inputs={\n",
    "        \"train_data\": Input(type=\"uri_folder\"),\n",
    "        \"test_data\": Input(type=\"uri_folder\"),\n",
    "        \"learning_rate\": Input(type=\"number\"),\n",
    "        \"registered_model_name\": Input(type=\"string\")\n",
    "    },\n",
    "    outputs=dict(\n",
    "        model=Output(type=\"uri_folder\", mode=\"rw_mount\")\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code='azure/components/train/',\n",
    "    command=\"\"\"python gbr.py \\\n",
    "              --train_data ${{inputs.train_data}} \\\n",
    "              --test_data ${{inputs.test_data}} \\\n",
    "              --learning_rate ${{inputs.learning_rate}} \\\n",
    "              --registered_model_name ${{inputs.registered_model_name}} \\\n",
    "              --model ${{outputs.model}}\n",
    "            \"\"\",\n",
    "    environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the inputs are specified exactly as there before. The script is also parameterized the same way. The output configuration is set to be a folder where the model object will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_data': {'type': 'uri_folder'},\n",
       " 'test_data': {'type': 'uri_folder'},\n",
       " 'learning_rate': {'type': 'number'},\n",
       " 'registered_model_name': {'type': 'string'}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_train.component.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': {'type': 'uri_folder', 'mode': 'rw_mount'}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step_train.component.outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have two components that are executable on their own but are not connected to each other. Here is where the `pipeline` function comes into play."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assemble Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    compute='c002',\n",
    "    description=\"data preparation and training pipeline\"\n",
    ")\n",
    "def diamond_prices_pipeline(\n",
    "    pipeline_job_data_input,\n",
    "    pipeline_job_test_train_ratio,\n",
    "    pipeline_job_learning_rate,\n",
    "    pipeline_job_registered_model_name,\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = step_process(\n",
    "        data=pipeline_job_data_input,\n",
    "        test_train_ratio=pipeline_job_test_train_ratio,\n",
    "    )\n",
    "\n",
    "    # using train_func like a python call with its own inputs\n",
    "    train_job = step_train(\n",
    "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
    "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
    "        learning_rate=pipeline_job_learning_rate,  # note: using a pipeline input as parameter\n",
    "        registered_model_name=pipeline_job_registered_model_name,\n",
    "    )\n",
    "\n",
    "    # a pipeline returns a dictionary of outputs\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
    "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how this function converts each of the steps defined before to be jobs by providing the values for the parameters. For the training job the component attributes are accessed to enable connections between components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "registered_model_name = \"diamond_prices_model_v1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = diamond_prices_pipeline(\n",
    "    pipeline_job_data_input=Input(type=\"uri_file\", path=diamond_prices.path),\n",
    "    pipeline_job_test_train_ratio=0.2,\n",
    "    pipeline_job_learning_rate=0.1,\n",
    "    pipeline_job_registered_model_name=registered_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"Training pipeline with registered components\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a pipeline job in Azure ML creates a UI where the progress of the job can be tracked. Logs could also be streamed from the pipeline like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: patient_fig_wc9sd73mqr\n",
      "Web View: https://ml.azure.com/runs/patient_fig_wc9sd73mqr?wsid=/subscriptions/5bcad9c4-40fb-4136-b614-cc90116dd8b3/resourcegroups/tf/workspaces/cloud-teach\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: patient_fig_wc9sd73mqr\n",
      "Web View: https://ml.azure.com/runs/patient_fig_wc9sd73mqr?wsid=/subscriptions/5bcad9c4-40fb-4136-b614-cc90116dd8b3/resourcegroups/tf/workspaces/cloud-teach\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have built and executed a single pipeline. While this pipeline can be executed on demand, what if we want it to be triggered automatically whenever any component within the pipeline changes? This includes changes in hyperparameter values or evaluation metrics.\n",
    "\n",
    "To achieve automatic execution upon detecting changes in the pipeline, we need mechanisms to identify specific modifications and define actions to be taken when changes are detected. Let's now focus on the initial aspect of this challenge: tracking changes in the pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version Control with `git`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version control is essential in an MLOps workflow for several reasons.\n",
    "\n",
    "a. Collaboration and Teamwork:\n",
    "   - In machine learning projects, multiple team members often work together on different aspects, such as data preprocessing, model development, and evaluation. Version control systems, like Git, enable seamless collaboration by allowing team members to work concurrently on the same project without conflicts.\n",
    "   - Git allows individuals to work on their own branches, making it easy to merge changes from different team members. It provides a centralized platform for coordination, code reviews, and efficient collaboration.\n",
    "\n",
    "b. Traceability and Reproducibility:\n",
    "   - MLOps workflows require traceability and reproducibility to ensure transparency and maintain the integrity of the pipeline. Version control systems provide a mechanism to track changes made to the code, data, and configurations over time.\n",
    "   - With Git, each commit represents a specific version of the codebase, making it possible to trace the evolution of the pipeline. This traceability ensures that the entire history of changes is available, enabling reproducibility and error investigation.\n",
    "\n",
    "c. Experimentation and Iteration:\n",
    "   - Machine learning involves experimentation and iteration to improve model performance. Version control systems facilitate the management and tracking of different experiments, enabling teams to compare different approaches and analyze their impact on the model.\n",
    "   - With Git, each experiment can be treated as a separate branch or commit, allowing for easy comparison and rollback if necessary. This ability to iterate quickly and maintain a historical record of experiments is crucial for optimizing models and achieving desired results.\n",
    "\n",
    "d. Rollbacks and Bug Fixes:\n",
    "   - In complex ML pipelines, issues and bugs are inevitable. Version control systems offer the capability to revert to previous working states, providing a safety net for rollbacks and bug fixes.\n",
    "   - Git allows teams to roll back to a specific commit or create a new branch to fix issues while preserving the integrity of the codebase. It ensures that previous working versions can be easily restored, preventing potential disruptions in the pipeline."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `clone` - `commit` - `push` workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![git-workflow](assets/git-workflow.drawio.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clone-commit-push workflow is a common Git workflow for working with remote repositories. It involves cloning a repository, making changes, committing those changes, and pushing them back to the remote repository. Here's a step-by-step explanation with examples:\n",
    "\n",
    "1. Clone the Repository:\n",
    "   - To clone a remote repository, use the `git clone` command followed by the repository's URL.\n",
    "   - Example:\n",
    "     ```\n",
    "     git clone https://github.com/example-user/example-repo.git\n",
    "     ```\n",
    "   - This creates a local copy of the remote repository on your machine.\n",
    "\n",
    "2. Make Changes:\n",
    "   - Change into the cloned repository's directory:\n",
    "     ```\n",
    "     cd example-repo\n",
    "     ```\n",
    "   - Make the necessary changes to the files in the repository using any text editor or IDE.\n",
    "\n",
    "3. Commit Changes:\n",
    "   - Stage the changes you want to commit using the `git add` command.\n",
    "   - Example:\n",
    "     ```\n",
    "     git add modified_file.py\n",
    "     ```\n",
    "   - Commit the staged changes with a meaningful commit message using the `git commit` command.\n",
    "   - Example:\n",
    "     ```\n",
    "     git commit -m \"Update modified_file.py with new feature\"\n",
    "     ```\n",
    "\n",
    "4. Push Changes:\n",
    "   - Push the committed changes to the remote repository using the `git push` command.\n",
    "   - Example:\n",
    "     ```\n",
    "     git push origin master\n",
    "     ```\n",
    "   - This command pushes the committed changes to the `master` branch of the remote repository named `origin`.\n",
    "\n",
    "Note: The `origin` is the default name of the remote repository. You can replace it with the appropriate name if your remote repository has a different name.\n",
    "\n",
    "5. Pull Changes (Optional):\n",
    "   - If you are working in a team or collaborating with others, it's a good practice to pull the latest changes from the remote repository before making your own changes.\n",
    "   - Use the `git pull` command to fetch and merge the latest changes from the remote repository.\n",
    "   - Example:\n",
    "     ```\n",
    "     git pull origin master\n",
    "     ```\n",
    "   - This ensures that your local repository is up to date with the remote repository before you make your own modifications.\n",
    "\n",
    "The clone-commit-push workflow allows you to work on your local repository, make changes, commit them with informative messages, and push them back to the remote repository. It facilitates collaboration, version control, and the seamless integration of changes into the project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Branching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![git-branches](assets/git-branch.drawio.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a collaborative development environment where multiple developers are working on different aspects of a proposed change, Git branching is a valuable feature that enables an organized and efficient workflow. Each developer can work on a separate branch, allowing them to make independent changes without interfering with each other's work. Here's an explanation of the usage of Git branching in this context:\n",
    "\n",
    "1. Creating Branches:\n",
    "   - Each developer starts by creating their own branch, typically based on the main branch (e.g., \"master\" or \"main\").\n",
    "   - Developers can use the `git branch` command to create a new branch or the `git checkout -b` command to create and switch to a new branch in one step.\n",
    "   - Example:\n",
    "     ```\n",
    "     git branch feature-branch\n",
    "     ```\n",
    "     or\n",
    "     ```\n",
    "     git checkout -b feature-branch\n",
    "     ```\n",
    "\n",
    "2. Working on Branches:\n",
    "   - Each developer now works on their respective branch, focusing on their specific tasks or changes.\n",
    "   - They can make changes, add new features, fix bugs, or modify code without affecting the main branch or other developers' work.\n",
    "   - Developers commit their changes locally to their branch using the standard `git add` and `git commit` commands.\n",
    "\n",
    "3. Sharing Branches:\n",
    "   - Developers can push their local branches to a shared remote repository to collaborate with others.\n",
    "   - They use the `git push` command with the branch name and the remote repository to push their branch.\n",
    "   - Example:\n",
    "     ```\n",
    "     git push origin feature-branch\n",
    "     ```\n",
    "\n",
    "4. Reviewing and Merging Changes:\n",
    "   - Once a developer has completed their work on the branch, they can create a pull request or merge request, depending on the Git hosting platform being used (e.g., GitHub, GitLab, Bitbucket).\n",
    "   - The pull request allows other developers to review the changes, provide feedback, and discuss the proposed changes before merging them into the main branch.\n",
    "   - After the review and approval process, the changes from the branch can be merged into the main branch using the platform's interface.\n",
    "\n",
    "5. Updating Branches:\n",
    "   - During the development process, other developers may make changes to the main branch.\n",
    "   - To incorporate those changes into their branch, developers can perform a branch update by switching to their branch and using the `git merge` command or `git rebase` command to integrate the latest changes from the main branch.\n",
    "   - Example:\n",
    "     ```\n",
    "     git checkout feature-branch\n",
    "     git merge main\n",
    "     ```\n",
    "\n",
    "By utilizing Git branching, developers can work on different aspects of a proposed change simultaneously, without interfering with each other's work. It allows for parallel development, easy collaboration, and the ability to review, discuss, and merge changes in an organized manner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- **SageMaker Pipelines**\n",
    "\n",
    "1. [Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/build-and-manage-steps.html)\n",
    "2. [Example 1](https://catalog.us-east-1.prod.workshops.aws/workshops/9a6bcca9-93d6-4e09-ada2-64b692267342/en-US/pipelines)\n",
    "3. [Example 2]([Example](https://sagemaker-examples.readthedocs.io/en/latest/sagemaker-pipelines/tabular/abalone_build_train_deploy/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.html)) \n",
    "\n",
    "- **Azure Pipelines**\n",
    "\n",
    "1. [DSL Documentation](https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.dsl?view=azure-python#functions)\n",
    "2. [Example 1](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-create-component-pipeline-python?view=azureml-api-2) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
